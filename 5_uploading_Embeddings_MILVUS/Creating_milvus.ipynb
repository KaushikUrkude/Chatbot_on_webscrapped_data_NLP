{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedded_vectors.pkl', 'rb') as f:\n",
    "    embedded_vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output11.txt\") as file:\n",
    "    corpus = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = [{'sentence': x, 'index' : i} for i, x in enumerate(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    # Go through each sentence dict\n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        # Create a string that will hold the sentences which are joined\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Add sentences before the current one, based on the buffer size.\n",
    "        for j in range(i - buffer_size, i):\n",
    "            # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
    "            if j >= 0:\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Add the current sentence\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Add sentences after the current one, based on the buffer size\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            # Check if the index j is within the range of the sentences list\n",
    "            if j < len(sentences):\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Then add the whole thing to your dict\n",
    "        # Store the combined sentence in the current sentence dict\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = combine_sentences(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_sentences = []\n",
    "\n",
    "for x in sentences:\n",
    "    com_sentences.append(x['combined_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes123 = np.load('indexes.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_above_thresh = indexes123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the start index\n",
    "start_index = 0\n",
    "\n",
    "# Create a list to hold the grouped sentences\n",
    "chunks = []\n",
    "\n",
    "# Iterate through the breakpoints to slice the sentences\n",
    "for index in indices_above_thresh:\n",
    "    # The end index is the current breakpoint\n",
    "    end_index = index\n",
    "\n",
    "    # Slice the sentence_dicts from the current start index to the end index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    \n",
    "    # Update the start index for the next group\n",
    "    start_index = index + 1\n",
    "\n",
    "# The last group, if any sentences remain\n",
    "if start_index < len(sentences):\n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_vectors = embedded_vectors.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(chunks)-1, -1, -1):\n",
    "    if len(chunks[i]) > 10000:\n",
    "        chunks.pop(i)\n",
    "        embedded_vectors.pop(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    FieldSchema,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'// 4. Compute saxpy and write back to shared memory\\nfor (int i = threadIdx.x; i < buf_len; i += blockDim.x) {\\n// 5. Wait for shared memory writes to be visible to TMA engine. // After syncthreads, writes by all threads are visible to TMA engine. // 6. Initiate TMA transfer to copy shared memory to global memory\\n// 7. Wait for TMA transfer to have finished reading shared memory. // Create a \"bulk async-group\" out of the previous bulk copy operation. // Wait for the group to have completed reading from shared memory. Barrier initialization. The barrier is initialized with the number of\\nthreads participating in the block. As a result, the barrier will flip only if\\nall threads have arrived on this barrier. Shared memory barriers are described\\nin more detail in Asynchronous Data Copies using cuda::barrier. To make the initialized barrier visible to subsequent bulk-asynchronous copies, the\\nfence.proxy.async.shared::cta instruction is used. This instruction ensures that\\nsubsequent bulk-asynchronous copy operations operate on the initialized barrier. TMA read. The bulk-asynchronous copy instruction directs the\\nhardware to copy a large chunk of data into shared memory, and to update the\\nof the shared memory barrier after completing the read. In general, issuing as\\nfew bulk copies with as big a size as possible results in the best performance. Because the copy can be performed asynchronously by the hardware, it is not\\nnecessary to split the copy into smaller chunks. The thread that initiates the bulk-asynchronous copy operation arrives at the barrier\\nusing mbarrier.expect_tx. This is automatically performed by cuda::memcpy_async. This tells the barrier that the thread has\\narrived and also how many bytes (tx / transactions) are expected to arrive. Only\\na single thread has to update the expected transaction count. If multiple\\nthreads update the transaction count, the expected transaction will be the sum\\nof the updates. The barrier will only flip once all threads have arrived and\\nall bytes have arrived. Once the barrier has flipped, the bytes are safe to read\\nfrom shared memory, both by the threads as well as by subsequent\\nbulk-asynchronous copies. More information about barrier transaction accounting\\nBarrier wait. Waiting for the barrier to flip is done using\\nmbarrier.try_wait. It can either return true, indicating that the wait is\\nover, or return false, which may mean that the wait timed out. The while loop\\nSMEM write and sync. The increment of the buffer values reads and writes to shared\\nmemory. To make the writes visible to subsequent bulk-asynchronous copies, the\\nfence.proxy.async.shared::cta instruction is used. This orders the writes to\\nshared memory before subsequent reads from bulk-asynchronous copy operations,\\nwhich read through the async proxy. So each thread first orders the writes to\\nobjects in shared memory in the async proxy via the\\nfence.proxy.async.shared::cta, and these operations by all threads are\\nordered before the async operation performed in thread 0 using\\nTMA write and sync. The write from shared to global memory is again\\ninitiated by a single thread. The completion of the write is not tracked by a\\nshared memory barrier. Instead, a thread-local mechanism is used. Multiple\\nwrites can be batched into a so-called bulk async-group. Afterwards, the\\nthread can wait for all operations in this group to have completed reading from\\nshared memory (as in the code above) or to have completed writing to global\\nmemory, making the writes visible to the initiating thread. For more information,\\nrefer to the PTX ISA documentation of cp.async.bulk.wait_group. Note that the bulk-asynchronous and non-bulk asynchronous copy instructions have\\nThe bulk-asynchronous instructions have specific alignment requirements on their source and\\ndestination addresses. More information can be found in the table below. Table 7 Alignment requirements for one-dimensional bulk-asynchronous operations in Compute Capability 9.0. Must be 8 byte aligned (this is guaranteed by cuda::barrier). The primary difference between the one-dimensional and multi-dimensional case is\\nthat a tensor map must be created on the host and passed to the CUDA kernel. This section describes how to create a tensor map using the CUDA driver API, how\\nto pass it to device, and how to use it on device. Driver API. A tensor map is created using the cuTensorMapEncodeTiled\\ndriver API. This API can be accessed by linking to the driver directly\\nAPI. Below, we show how to get a pointer to the cuTensorMapEncodeTiled API. For more information, refer to Driver Entry Point Access. // Use cuGetProcAddress to get a pointer to the CTK 12.0 version of cuTensorMapEncodeTiled\\nCUresult res = cuGetProcAddress(\"cuTensorMapEncodeTiled\", &cuTensorMapEncodeTiled_ptr, 12000, CU_GET_PROC_ADDRESS_DEFAULT, &symbol_status);\\nCreation. Creating a tensor map requires many parameters. Among\\nthem are the base pointer to an array in global memory, the size of the array\\n(in number of elements), the stride from one row to the next (in bytes), the\\nsize of the shared memory buffer (in number of elements). The code below creates\\na tensor map to describe a two-dimensional row-major array of size GMEM_HEIGHT\\nx GMEM_WIDTH. Note the order of the parameters: the fastest moving dimension\\n// rank is the number of dimensions of the array. // The stride is the number of bytes to traverse from the first element of one row to the next. // It must be a multiple of 16.\\nuint64_t stride[rank - 1] = {GMEM_WIDTH * sizeof(int)};\\n// The box_size is the size of the shared memory buffer that is used as the\\n// The distance between elements in units of sizeof(element). A stride of 2\\n// can be used to load only the real component of a complex-valued tensor, for instance.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "from concurrent import futures\n",
    "\n",
    "# Create a gRPC server with increased max message length\n",
    "server = grpc.server(\n",
    "    futures.ThreadPoolExecutor(max_workers=10),\n",
    "    options=[('grpc.max_send_message_length', 100 * 1024 * 1024), \n",
    "             ('grpc.max_receive_message_length', 100 * 1024 * 1024)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import Collection, CollectionSchema, FieldSchema, DataType\n",
    "\n",
    "# Define the schema\n",
    "fields = [\n",
    "    FieldSchema(name=\"SrNo\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"combined_chunks\", dtype=DataType.VARCHAR,max_length=10000),\n",
    "    FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields, \"Nvidia_CUDA_Chunked_embeddings\")\n",
    "\n",
    "# Create the collection\n",
    "collection = Collection(\"N_cuda\", schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_nos = [None] * 40763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [chunks,embedded_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for start in range(0, len(chunks), batch_size):\n",
    "    end = min(start + batch_size, len(chunks))\n",
    "    batch_combined_chunks = chunks[start:end]\n",
    "    batch_embeddings = embedded_vectors[start:end]\n",
    "    data = [chunks, embedded_vectors]\n",
    "    collection.insert(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Milvus(host='localhost', port='19530')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = 'N_cuda'\n",
    "\n",
    "# Load the collection\n",
    "collection = Collection(collection_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
